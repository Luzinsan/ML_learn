> MLP - multilayer persceptron - это способ преодоления органичений линейных моделей - он включает дополнительно 1 скрытый слой.
> Полносвязный MLP с 5 нейронами (5 hidden units) в скрытом слое: (пример состоит из 2х слоёв: скрытый и выходной)
>  ![[MLP.png]]
> где первые L-1 слоя - representation
> последний слой - linear predictor 

# MLP
$$\begin{aligned}
    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.
\end{aligned}$$
где $\mathbf{X} \in \mathcal{R}^{n \times d}$ - это minibatch из $n$ наблюдений с $d$ признаками
$\mathbf{H} \in \mathcal{R}^{n \times h}$ - это слой скрытых нейронов из $n$ наблюдений с $h$ признаками (*hidden representation*)
$\mathbf{W}^{(1)} \in \mathcal{R}^{d \times h}$ - веса для получения нейронов скрытого слоя (из пространства признаков $d$-размерности в $h$-размерность)
и $\mathbf{b} \in \mathcal{R}^{1 \times h}$ - смещение для получения нейроной скрытого слоя
$\mathbf{W}^{(2)} \in \mathcal{R}^{h \times q}$ - веса для получения нейронов выходного слоя (результирующие вероятности)
и $\mathbf{b} \in \mathcal{R}^{1 \times q}$ - смещение для получения нейронов выходного слоя
> Т.е. $\mathbf{W}^{(1)}$ и $\mathbf{W}^{(2)}$ - это, по сути [[Композиция матриц]]
> > Скрытые модули задаются афинной функцией входных данных, а выходные данные есть афинная функция скрытых модулей => аффинная функция аффинной функции - это *аффинная функция*!
> > Эквивалентная линейная модель с 1 слоем: $\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$ и $\mathbf{b}=\mathbf{b}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}$ 
> > Подставляем: = $\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.$

# [[Функции активации]]