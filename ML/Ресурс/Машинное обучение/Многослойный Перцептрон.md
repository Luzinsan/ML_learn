> MLP - multilayer persceptron - это способ преодоления органичений линейных моделей - он включает дополнительно 1 скрытый слой.
> Полносвязный MLP с 5 нейронами (5 hidden units) в скрытом слое: (пример состоит из 2х слоёв: скрытый и выходной)
>  ![[Pasted image 20240224140734.png]]
> где первые L-1 слоя - representation
> последний слой - linear predictor 

# MLP
$$\begin{aligned}
    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.
\end{aligned}$$
где $\mathbf{X} \in \mathcal{R}^{n \times d}$ - это minibatch из $n$ наблюдений с $d$ признаками
$\mathbf{H} \in \mathcal{R}^{n \times h}$ - это слой скрытых нейронов из $n$ наблюдений с $h$ признаками (*hidden representation*)
$\mathbf{W}^{(1)} \in \mathcal{R}^{d \times h}$ - веса для получения нейронов скрытого слоя (из пространства признаков $d$-размерности в $h$-размерность)
и $\mathbf{b} \in \mathcal{R}^{1 \times h}$ - смещение для получения нейроной скрытого слоя
$\mathbf{W}^{(2)} \in \mathcal{R}^{h \times q}$ - веса для получения нейронов выходного слоя (результирующие вероятности)
и $\mathbf{b} \in \mathcal{R}^{1 \times q}$ - смещение для получения нейронов выходного слоя
> Т.е. $\mathbf{W}^{(1)}$ и $\mathbf{W}^{(2)}$ - это, по сути [[Композиция матриц]]
> > Скрытые модули задаются афинной функцией входных данных, а выходные данные есть афинная функция скрытых модулей => аффинная функция аффинной функции - это *аффинная функция*!
> > Эквивалентная линейная модель с 1 слоем: $\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$ и $\mathbf{b}=\mathbf{b}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}$ 
> > Подставляем: = $\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.$

# Функции активации
$$
\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}
$$
> Чтобы реализовать многослойные архитектуры, которые не будут комбинацией трансформаций (т.е. не будут сводиться до линейных моделей), ключевой элемент - нелинейная функция активации $\sigma$
> *Activation function* применяется к каждому нейрону в слое после аффинного преобразования.
> Activations - это выходные данные функции активации 

- Функции активации решают, следует ли активировать нейрон или нет, путём расчета взвешенной суммы и добавления к ней смещения
- Являются дифференциируемыми операторами преобразования входных сигналов в выходные.

## ReLU
![[Pasted image 20240224155231.png]]![[Pasted image 20240224160457.png]]
$$\sigma(x) = ReLU(x)  = max(0,x)$$
> **Re**ctified **L**inear **U**nit - выпремленная линейная единица
> Применяется **поэлементно** (а не построчно) к выходам $x$ 
> Хорошая производительность при решении задач предсказания/прогнозирования.
> Неформально: ReLU сохраняет только положительные элементы и отбрасывает все отрицательные элементы, устанавливая им активации равные нулю.

```python
# ReLU
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```

```python
# ReLU'x
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
```

>> Проще поддаётся оптимизации, чем $Sigmoid$ или $Tanh$. 
>
### pReLU
$$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).$$
> Параметризованная функция ReLU
> Добавляет линейный скаляр $\alpha$ так, что некоторая информация все равно проходит, даже если аргумент отричательный

## Sigmoid Function
![[Pasted image 20240224173435.png]]![[Pasted image 20240224171916.png]]![[Pasted image 20240224174337.png]]![[Pasted image 20240224183243.png]]
**$Sigmoid(x)$** $$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$
**$\frac{d}{dx}sigmoid(x)$** $$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).$$

> Функция сжатия - преобразует $x \in \mathcal{R}$ => $\mathcal{R[0,1]}$
> И активация нейрона происходит только тогда, когда значение сигмоиды больше порогового значения (thresh).
> Является дифференциируемой функцией
> В некотором роде, является частным случаем ==Softmax== функции: [[Softmax Regression#Softmax - функция активации]]

```python
# sigmoid
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))

# sigmoid'x
# Clear out previous gradients
x.grad.data.zero_()
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
```

> Когда входной сигнал равен 0 - производная $sigmoid$ функции достигает максимума `0,25`, т.к. входные данные расходятся от 0 в любом направлении.

> Минусы: создаёт проблемы в оптимизации - градиент исчезает при больших положительных и отрицательных аргументах, что приводит к плато, из которого сложно выбраться (поэтому часто в скрытых слоях применяют именно ReLU)
> Сигмоиды часто применяют в архитектурах [[Рекуррентные нейронные сети|Рекуррентных нейронных сетей]] (которые используют сигмоиды для управления потоком информации во времени) 


## Tanh Functions
![[Pasted image 20240224183949.png]]![[Pasted image 20240224184544.png]]
$tanh$
$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$
$\frac{d}{dx}tanh(x)$   $$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$

> Тангенс функция - *гиперболический* *тангенс* - **функция сжатия**
> Преобразует аргументы $x \in \mathcal{R}$ => $\mathcal{R}^{[-1,1]}$ 
> В отличие от сигмоидной функии - $tanh$ симметрична началу координат

```python
# tanh
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))

# tanh'x
# Clear out previous gradients
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
```
## GELU
> **Gaussian Error Linear Unit** - Линейная единица гауссовой ошибки - функция активации $x \Phi(x)$ by `Hendrycks.Gimpel.2016`
> где $\Phi(x)$ - стандартная функция кумулятивного распределения *Гаусса* и *Гимпела*.

## Switch function
$\sigma(x) = x \operatorname{sigmoid}(\beta x)$
> `Ramachandran.Zoph.Le.2017`
> Может обеспечить более высокую точность