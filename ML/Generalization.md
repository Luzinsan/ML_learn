В обучении с учителем есть строгое предположение - независимость от идентичных распределений - independently from identical distributions.
Согласно IID считается, что обучая модель на данных распределения P(X,Y), мы можем предсказывать данные из множества Q(X,Y), *идентичного* P.

---
- Training error - $R_{emp}$ - статистическая величина, вычисленная на тренировочном датасете.
> $$R_\textrm{emp}[\mathbf{X}, \mathbf{y}, f] = \frac{1}{n} \sum_{i=1}^n l(\mathbf{x}^{(i)}, y^{(i)}, f(\mathbf{x}^{(i)})),$$
- Generalization error - $R$ - мат.ожидание базового распределения.
> Базовое распределение - распределение, возникаемое из непрерывного потока данных - **истинное распределение**.
> $$R[p, f] = E_{(\mathbf{x}, y) \sim P} [l(\mathbf{x}, y, f(\mathbf{x}))] =

\int \int l(\mathbf{x}, y, f(\mathbf{x})) p(\mathbf{x}, y) \;d\mathbf{x} dy.$$- $p(\mathbf{x},y)$ - функция плотности базового распределения - неизвестна (весь мы не можем обозреть все данные в мире)
> Решение: мы должны *оценить* $R$, применив модель к независимому тестовому набору ($\mathbf{X}', \mathbf{y}'$) - приходим к понятию validation error (ошибки проверки)

---
> > Низкая ошибка обучения - необходимый, но недостаточный показатель качества обобщения модели.
---
- Недообучение - Underfitting - когда ошибки обучения и валидации большие, но между ними также есть разрыв (необязательно большой)
- Переобучение - Overfitting - когда ошибка обучения значительно ниже, чем ошибка валидации. Переобучение - не всегда плохо.
  ![[Pasted image 20240220105300.png]]
  # Cross-Validation
  > Когда тренировочного датасета мало, нам может быть недостаточно данных для выделения валидационного датасета.
  > Решение: K-fold cross-validation.
  > Принцип: исходный датасет разбирается на K неперекрывающихся подмножеств.
  > Тренировка и валидация выполняются K раз. Каждый раз модель обучается на K-1 подмножестве, а валидация происходит на оставшемся кусочке. 
  > Оценки обучения и валидации оцениваются путем математического усреднения результатов.
  
  # Основные идеи
  - Использовать K-fold cross-validation для выбора модели.
  - Для более сложных моделей часто требуется больше данных.
  - Понятия сложности включают в себя как кол-во параметров, так и диапазон значений, которые они могут принимать.
  - При прочих разных условиях большее кол-во данных почти всегда приводит к лучшей генерализации (обобщению).
  - Генерализация основана на предположении IID.