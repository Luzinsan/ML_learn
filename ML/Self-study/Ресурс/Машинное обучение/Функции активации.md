
$$
\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}
$$
> Чтобы реализовать многослойные архитектуры, которые не будут комбинацией трансформаций (т.е. не будут сводиться до линейных моделей), ключевой элемент - нелинейная функция активации $\sigma$
> *Activation function* применяется к каждому нейрону в слое после аффинного преобразования.
> Activations - это выходные данные функции активации 

- Функции активации решают, следует ли активировать нейрон или нет, путём расчета взвешенной суммы и добавления к ней смещения
- Являются дифференциируемыми операторами преобразования входных сигналов в выходные.

# ReLU
![[ReLU.png]]![[grad ReLU.png]]
$$\sigma(x) = ReLU(x)  = max(0,x)$$
> **Re**ctified **L**inear **U**nit - выпремленная линейная единица
> Применяется **поэлементно** (а не построчно) к выходам $x$ 
> Хорошая производительность при решении задач предсказания/прогнозирования.
> Неформально: ReLU сохраняет только положительные элементы и отбрасывает все отрицательные элементы, устанавливая им активации равные нулю.

```python
# ReLU
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```

```python
# ReLU'x
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
```

>> Проще поддаётся оптимизации, чем $Sigmoid$ или $Tanh$. 
>
## pReLU
$$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).$$
> Параметризованная функция ReLU
> Добавляет линейный скаляр $\alpha$ так, что некоторая информация все равно проходит, даже если аргумент отричательный

# Sigmoid Function
![[sigmoid manual.png]]![[sigmoid.png]]![[sigmoid thresh.png]]![[grad sigmoid.png]]
**$Sigmoid(x)$** $$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$
**$\frac{d}{dx}sigmoid(x)$** $$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).$$

> Функция сжатия - преобразует $x \in \mathcal{R}$ => $\mathcal{R[0,1]}$
> И активация нейрона происходит только тогда, когда значение сигмоиды больше порогового значения (thresh).
> Является дифференциируемой функцией
> В некотором роде, является частным случаем ==Softmax== функции: [[Softmax Regression#Softmax - функция активации]]

```python
# sigmoid
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))

# sigmoid'x
# Clear out previous gradients
x.grad.data.zero_()
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
```

> Когда входной сигнал равен 0 - производная $sigmoid$ функции достигает максимума `0,25`, т.к. входные данные расходятся от 0 в любом направлении.

> Минусы: создаёт проблемы в оптимизации - градиент исчезает при больших положительных и отрицательных аргументах, что приводит к плато, из которого сложно выбраться (поэтому часто в скрытых слоях применяют именно ReLU)
> Сигмоиды часто применяют в архитектурах [[Рекуррентные нейронные сети|Рекуррентных нейронных сетей]] (которые используют сигмоиды для управления потоком информации во времени) 


# Tanh Functions
![[tanh.png]]![[grad tanh.png]]
$tanh$
$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$
$\frac{d}{dx}tanh(x)$   $$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$

> Тангенс функция - *гиперболический* *тангенс* - **функция сжатия**
> Преобразует аргументы $x \in \mathcal{R}$ => $\mathcal{R}^{[-1,1]}$ 
> В отличие от сигмоидной функии - $tanh$ симметрична началу координат

```python
# tanh
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))

# tanh'x
# Clear out previous gradients
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
```
# GELU
> **Gaussian Error Linear Unit** - Линейная единица гауссовой ошибки - функция активации $x \Phi(x)$ by `Hendrycks.Gimpel.2016`
> где $\Phi(x)$ - стандартная функция кумулятивного распределения *Гаусса* и *Гимпела*.

# Switch function
$\sigma(x) = x \operatorname{sigmoid}(\beta x)$
> `Ramachandran.Zoph.Le.2017`
> Может обеспечить более высокую точность