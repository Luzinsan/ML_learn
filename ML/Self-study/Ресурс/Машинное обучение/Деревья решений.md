# Эвристики - критерии разделения узлов

![[Энтропия]]
### В задаче бинарной классификации
$$S = -p_{+}\log_2{p_{+}} - p_- \log_2 p = -p_+ \log_2 p_+ - (1 - p_+)\log_2 (1 - p_+)$$
## Tree-building Algorithm
- Принцип: жадная максимизация прироста информации (Greedy maximization of information gain): на каждом шаге алгоритм выбирает переменную, дающую наибольший прирост информации при расщеплении.
- Процедура повторяется рекурсивно до тех пор, пока энтропия не будет равна значению, близкому к нулю, но позволяющему предотвратить переобучение.
	- Различные алгоритмы используют разные эвристики для "ранней остановки" или "отсечения"
```pcode
def build(L):
    create node t
    if the stopping criterion is True:
        assign a predictive model to t
    else:
        Find the best binary split L = L_left + L_right
        t.left = build(L_left)
        t.right = build(L_right)
    return t  
```
## 2. Gini uncertainty (Gini impurity) - Непределённость Джини - default в sklearn
$$G = 1 - \sum_k{p_k}^2$$
> Максимизация этого критерия - это максимизация кол-ва пар объектов одного класса, находящихся в одном поддереве.
> Работает схожим образом, как и information gain (прирост информации).

### В задачах бинарной классификации
$$G = 1 - p^2_+ - p^2_- = 1 - p^2_+ - (1- p_+)^2 =2 p_+(1 - p_+)$$
## 3. Misclassification error - Ошибка неправильной классификации
$$E = 1 - \max_k{p_k}$$
> Используется очень редко

# Выбор порогового значения для числовых признаков
- Принцип: сортировка значений признака по возрастанию; фиксируются средние значения между значениями признака, в которых целевой класс "переключается" (например с 1 на 0, и наоборот)
- Если присутсвует несколько числовых признаков - дерево выбирает то значение порога и признак, которое даёт лучшее разделение (согласно выбранному критерию: энтропия Шеннона или неопределенность Джини (по умолчанию)) 
## Глубина дерева
> Обычно максимальная грубина дерева приводит к переобучению (overfitting)

Исключения, при которых деревья строятся в максимальную глубину:
- Случайный лес (Random Forest) - усредняет отклики отдельных деревьев, построенных на максимальную глубину
- Обрезка деревьев (Pruning trees) - дерево сначала строится на максимальную глубину, затем снизу удаляются некоторые узлы дерева, сравнивая качестве дерева с разбиением и без (сравнение с помощью кросс-валидации)
### Предотвращение переобучения дерева
- настройка гиперпараметров: глубина дерева или кол-во наблюдений в листиях
- Обрезка деревьев (Pruning trees)


# Пример использования из sklearn
## Классификация
```python
from sklearn.tree import DecisionTreeClassifier

clf_tree = DecisionTreeClassifier(criterion="entropy", 
								  max_depth=3, random_state=17)

# training the tree
clf_tree.fit(train_data, train_labels)
```
- `criterion` - можно определить используемую эвристику для разделения узлов
- `max_depth` - максимальная длина дерева, настраиваемая для предотвращения переобучения
- `random_state` - для повторного воспроизведения обучения
- `fit` - вызывает процесс подбора модели, параметры это матрицы n строк-наблюдений на m столбцов-признаков (или целевых значений). В противном случае нужно транспотировать матрицу.
```python
# Let’s write an auxiliary function that will return grid for further visualization.
def get_grid(data):
    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1
    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1
    return np.meshgrid(np.arange(x_min, x_max, 0.01), 
				       np.arange(y_min, y_max, 0.01))
    
# some code to depict separating surface
xx, yy = get_grid(train_data)
predicted = clf_tree.predict(np.c_[xx.ravel(),
								   yy.ravel()]).reshape(xx.shape)
plt.pcolormesh(xx, yy, predicted, cmap="autumn")
plt.scatter(
    train_data[:, 0],
    train_data[:, 1],
    c=train_labels,
    s=100,
    cmap="autumn",
    edgecolors="black",
    linewidth=1.5,
);
```
- `get_grid(data)` - создание исскуственных тестовых данных (пространства задачи) для последующей отризовки в 2D плоскости
- `pcolormash` - отрисовка предсказаний
- `scatter` - отризовка тренировочных данных
```python
import pydotplus 
from sklearn.tree import export_graphviz


def tree_graph_to_png(tree, feature_names, png_file_to_save):
    # needs graphviz to be installed
    tree_str = export_graphviz(
        tree, feature_names=feature_names, filled=True, out_file=None
    )
    graph = pydotplus.graph_from_dot_data(tree_str)
    graph.write_png(png_file_to_save)

tree_graph_to_png(
    tree=clf_tree,
    feature_names=["x1", "x2"],
    png_file_to_save="topic3_decision_tree1.png",
)
```
- Интерпретация модели
### Класс DecisionTreeClassifier
**Гиперпараметры:** (зависят от входных данных, используются совместно с кросс-валидацией)
- max_depth - максимальная глубина дерева
- max_features - максимальное кол-во признаков, по которым строится модель (разделяются узлы)
- min_sample_leaf - минимальное кол-во экземпляров в листе
## Регрессия
> Принцип построения дерева такой же, меняется критерий качества:
> $l$ - кол-во выборок в листе
> $y_i$ - значение целевой переменной
> > Принцип: минимизируем дисперсию, путем поиска таких признаков, которые делят обучающую выборку так, чтобы значения целевого признака в каждом листе были примерно одинаковы (с минимальной дисперсией)
> > Результат: решающее дерево будет аппроксимировать данные кусольно-постоянной функцией (ступеньками)

$$D = \frac{1}{l}\sum^l_{i=1}{(y_i - \frac{1}{l} \sum^l_{j=1}y_j)^2},$$
### Пример реализации
Регрессия данных, распределённых функцией: $f(x)=e^{-x^2}+1.5 * e^{-(x-2)^2}$
```python
n_train = 150
n_test = 1000
noise = 0.1


def f(x):
    x = x.ravel()
    return np.exp(-(x ** 2)) + 1.5 * np.exp(-((x - 2) ** 2))


def generate(n_samples, noise):
    X = np.random.rand(n_samples) * 10 - 5
    X = np.sort(X).ravel()
    y = (
        np.exp(-(X ** 2))
        + 1.5 * np.exp(-((X - 2) ** 2))
        + np.random.normal(0.0, noise, n_samples)
    )
    X = X.reshape((n_samples, 1))
    return X, y


X_train, y_train = generate(n_samples=n_train, noise=noise)
X_test, y_test = generate(n_samples=n_test, noise=noise)

from sklearn.tree import DecisionTreeRegressor

reg_tree = DecisionTreeRegressor(max_depth=5, random_state=17)

reg_tree.fit(X_train, y_train)
reg_tree_pred = reg_tree.predict(X_test)

plt.figure(figsize=(10, 6))
plt.plot(X_test, f(X_test), "b")
plt.scatter(X_train, y_train, c="b", s=20)
plt.plot(X_test, reg_tree_pred, "g", lw=2)
plt.xlim([-5, 5])
plt.title(
    "Decision tree regressor, MSE = %.2f"
    % (np.sum((y_test - reg_tree_pred) ** 2) / n_test)
)
plt.show()
```
![[Decision Tree Regressor.png|500]]
