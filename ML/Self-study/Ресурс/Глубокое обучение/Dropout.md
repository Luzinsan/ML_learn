![[Dropout.png]]
> Dropout - это внедрение шума при вычислении каждого внутреннего слоя во время [[Propagation#Forward Propagation (Forward Pass)|Forward Propagation]], в результате чего некоторые нейроны деактивируются, перед вычислением следующего слоя.

> Слой шума в работе Бишопа создавался из гауссова распределения. Далее эти значения добавляются к входу $\mathbf{x}$, получая возмущённую точку $\mathbf{x}'=\mathbf{x}+\epsilon$
> $E[\mathbf{x}']=\mathbf{x}$

# Standard dropout regulization
- Обнуляется некоторая доля узлов в каждом слое
- Каждый слой debiases путем нормализации на долю узлов, которые не были dropout
- Т.е. с вероятностью dropout $p$, каждая промежуточная активация $h$ заменяется случайной величиной $h'$: $$\begin{aligned}
h' =
\begin{cases}
    0 & \textrm{ with probability } p \\
    \frac{h}{1-p} & \textrm{ otherwise}
\end{cases}
\end{aligned}$$
- Среднее остаётся неизменным: $E[h']=h$

> Во время тестирования drouput отключается: имея обученную модель и предсказываемый пример, мы не должны отбрасывать ни одного узла, т.к. не нуждаемся в нормализации.