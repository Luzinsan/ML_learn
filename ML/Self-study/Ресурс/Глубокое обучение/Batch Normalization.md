> https://arxiv.org/pdf/1502.03167

- Стандартизация, приведение данных к нулевому среднему $\mu=0$ и единичной дисперсии $\sum=1$ обеспечивают хороший контроль над проблемой оценки, ограничивают сложность функции => помещает параметры в одинаковый масштаб.
- Отсутствие нормализации приводит к дрейфу в распределении (drift in the distribution) и припятствует конвергенции (сходимости) сети. Это как например один слой имеет значения ф.активаций в 100 превышающие активацию другого слоя. Один из способов борьбы: адаптивные оптимизаторы: Adam, AdaGrad, Yogi, Distributed Shampoo
- Более глубокие сети сложны и склонны к переобучению, поэтому необходима регуляризация. Одни из способов: введение шума, добавление dropout

Batch Normalization имеет все 3 преимущества: предварительную обработку, числовую стабильность (numerical stability) и регуляризацию.
- применяется к отдельным слоям или ко всем: на каждой итерации обучения входные данные пакета нормализуются (вычитается среднее и делится на std), масштабируются на коэффициент (для восстановление степеней свободы) и смещаются
- выбор размера пакета при этом подходе очень важен (гиперпараметр); больший размер пакета предпочтительнее (50-100).
- статистические значения для этого слоя различаются в зависимости от режима (похоже на [[Dropout]], но он отключается на этапе оценки): в тренировочном режиме среднее и отклонение вычисляется на основе пакета, в режиме прогнозирования - на основе всего датасета.
$$BN(\mathbf{x}) = \gamma \space \odot \frac{\mathbf{x}-\mathbf{\hat{\mu_{\mathcal{B}}}}}{\mathbf{\hat\sigma_\mathcal{B}}} + \beta$$
> где $\mathbf{\hat\mu_\mathcal{B}}$ - выборочное среднее
> $\hat\sigma_\mathcal{B}$ - выборочное стандартное отклонение
> $\gamma$ - коэффициент поэлементного масштабирования
> $\beta$ - параметр сдвига
- после применения мини-пакет имеет нулевое среднее, единичную дисперсию и восстановленную форму
- $\gamma$ и $\beta$ являются обучаемыми параметрами

Применение в зависимости от типа слоя различается:
- Для полносвязных слоев - после аффинного преобразования и перед нелинейной функцией активации: $\mathbf{h} = \phi(BN_\mathcal{B}(\mathbf{Wx} + \mathbf{b}))$ 
- Для сверточных слоев - по расположению слоя - также, но операция применяетсяя для каждого канала по всем местоположениям
