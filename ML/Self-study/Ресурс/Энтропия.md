> - количественная оценка информации, содержащейся в данных. Она накладывает ограничения на нашу способность сжимать данные. 
# Энтропия Шеннона
Для распределения $P$, энтропия Шеннона $H[P]$ (или $S$) определяется определяется для системы с $N$ возможными состояниями:
$$S = -\sum^N_{i=1}{p_i*\log_2{p_i}},$$где $p_i$ - вероятность нахождения системы в i-ом состоянии ($\frac{N_i}{N}$).
Энтропия позволяет описать степень хаоса (разрозненности, неупорядоченности) системы.
Чем выше энтропия, тем менее упорядочена система, и наоборот.
> Мы стремимся уменьшить энтропию.
> Уменьшение энтропии приводит к приросту информации 
> - **Information gain (IG)**. 
> $Q$ - это признак, по которому идёт разделение выборки.
> $q$ - кол-во групп после разделения
> $N_i$ - количество объектов подвыборки, в которых переменная $Q$ равна $i$
> $S_i$ - энтропия этой подвыборки

$$IG(Q) = S_0 - \sum^q_{i=1}{\frac{N_i}{N}*S_i},$$