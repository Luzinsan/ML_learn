> [[torch.nn]]


```python
import torch
```
> Чтобы узнать, какие функции и классы могут вызваны из модуля:
> `print(dir(torch.distributions))`
> Для получения конкретных инструкций:
> `help(torch.ones)`
> Отображение в $Jupyter\space Notebook$
> `torch.ones??` или `torch.ones?`

> Основа всего в библиотеке - тензор - то же самое, что и ndarray (из [[numpy]]), но с дополнительными killer-features: автоматическое дифференцирование, использование GPUs.
> Если тензор одномерный - его называют **вектором**.
> Каждое значение в тензоре - называется **элементом** тензора (или записью (*entire*), компонентой(*component*)).
> Если тензор состоит только их *одного* элемента - то это ***тензор-скаляр***

> Присвоение одного тензора другой переменной производит ***ссылку*** на элементы первого тензора.
> Присвоение результата бинарной операции создаёт новый объект (перевыделение памяти)
> `id(X)` - для получения адреса объекта
> `before = id(Y)`
> `Y = Y + X`
> `id(Y) == before`
> > **False**
> Чтобы избежать перевыделения памяти применяют индексацию `[]` или операторы `+=, -=, *=, /=` и т.п:
```python 
Z = torch.zeros_like(Y)
print('id(Z):', id(Z)) # id(Z): 140381179266448
Z[:] = X + Y
print('id(Z):', id(Z)) # id(Z): 140381179266448

before = id(X)
X += Y
id(X) == before # True
```
> Чтобы явно копировать, используют метод
> `clone()`
# Creating
- `torch.tensor(size)` - создание тензора вручную 
  Для двумерного списка: внешний список - ось 0, внетренний список - ось 1
- `torch.arange(length, dtype)` - создание вектора равномерно распределённых значений в промежутке `[0, n)`. 
  По умолчанию:
	  - Размер интервала - 1;
	  - Хранение тензоров - в основной памяти;
	  - Вычисление на базе CPU.
- `torch.zeros(size)` - создание тензора, заполненного нулями
- `torch.ones(size)` - создание тензора, заполненного единицами
- `torch.randn(size)` - создание тензора, заполненного значениями из гауссового распределения (std=0, мат.о.=1)
## Info
- `x.numel()` - возвратить общее кол-во элементов в тензоре
	- `len(x)` - возвращает размерность *нулевой* оси
- `x.shape` - возвратить объект torch.Size([]), предоставляющий информацию о форме тензора (длина вдоль каждой оси).
> Соглашения:
> > order - количество осей (измерений)
> > dimensions - размерность как количество компонент в оси
# Transforming
- `X.reshape(new_shape) = A` - изменить форму тензора, не меняя размер или значений
	- `X.reshape(-1, w)` или `X-reshape(h, -1)` - `-1` позволяет автоматически вывести длину оси, учитывая остальные данные
- `A.T` - транспонирование матрицы `A`
# Indexing and Slicing
- Аналогична индексации в python. Примеры:
```python
tensor([[2, 1, 4, 3], 
		[1, 2, 3, 4], 
		[4, 3, 2, 1]])
X[-1],  # tensor([ 4.,  3., 2., 1.]),
X[1:3], # tensor([[ 1.,  2.,  3.,  4.],
        #         [ 4.,  3., 2., 11.]])
```
- Доступное прямое изменение элементов через индексы/срезы
# Operations
## Унарные
> Унарные операции в математической нотации: $f: R$ $\rightarrow$ $R$  
- `torch.exp(x)` - отображает все значения тензора x в соответстующие значения функции экспоненты
  ![[Экспоненциальная.png|300]]
## Бинарные
> Бинарные операции в поэлементном виде: $f: R^2 \rightarrow R$
### Бинарные, like унарные поэлементные
- `x+y`
	- если x - это скаляр, к элементам y просто прибавляется скаляр
- `x-y`
- `x*y` 
	- Если x и y - это матрицы, то произведение называют произведением Адамара (поэлементное) 
- `x/y`
- `x**y`
- `x==y`
#### Broadcasting
> Broadcasting: если размерности левого и правого операнда не совпадают, то они расширяются путём репликации своих элементов до нужной размерности:
```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
# (tensor([[0],
#          [1],
#          [2]]),
# tensor([[0, 1]]))
a + b
# tensor([[0, 1],
#         [1, 2],
#         [2, 3]])
# a расширилися до # (tensor([[0, 0],
                   #          [1, 1],
                   #          [2, 2]])
# b расширился до  # (tensor([[0, 1],
                   #          [0, 1],
                   #          [0, 1]])
```

```python
A / sum_A
# tensor([[0., 1., 2.], 
#         [3., 4., 5.]])
# /
# tensor([[ 3.],     tensor([[ 3., 3.,   3.], 
#         [12.]]) =>         [12., 12., 12.]])
# = 
# tensor([[ 3., 4.,   5.], 
#         [15., 16., 17.]])
```
### Бинарные, like линейные алгебраические
- `torch.dot(x,y)` - **dot/inner product**: *exist* $\mathbf{x}, \mathbf{y} \in \mathbb{R}^d$ => $\langle \mathbf{x}, \mathbf{y} \rangle$ = $\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i$ - torch.sum(x * y) - *under the hood*
- `torch.mv(A, x)` - матрично-векторное произведение, являющееся по-сути скалярным произведением: $\mathbf{A}\in \mathbb{R}^{m \times n} * x \in \mathbb{R}^{n} = y \in \mathbb{R}^{m}$ 
  Функция: перевод *x* из n-мерного пространства в m-мерное
  $$
\mathbf{A}\mathbf{x}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix}\mathbf{x}
= \begin{bmatrix}
 \mathbf{a}^\top_{1} \mathbf{x}  \\
 \mathbf{a}^\top_{2} \mathbf{x} \\
\vdots\\
 \mathbf{a}^\top_{m} \mathbf{x}\\
\end{bmatrix}.
$$
- `torch.mm(A,B)` - матричное произведение [[Композиция матриц]]
  $$\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix}
\begin{bmatrix}
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
\end{bmatrix}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\
 \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\
 \vdots & \vdots & \ddots &\vdots\\
\mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m
\end{bmatrix}.
$$
- `A@x` & `A@B` - аналогично `mv(A, x)` и `mm(A,B)`
### Бинарные другие
- `torch.cat((X,Y), dim=0)` - конкатенация тензоров путём сложения их встык по некоторой оси (направлению-*`dimentional`*)
- `torch.stack((X,Y), dim=0)` - конкатенация тензоров с добавлением нового измерения в соответствии с параметром `dim`
## tensor($R^n$) $\rightarrow R$
-  `X.sum()` - суммирование всех элементов в тензоре 
	- $\sum^n_{i=1}{x_i}$  *if* $x \in R$
	- $\sum^m_{i=1}\sum^n_{j=1}{a_{ij}}$ *if* $x \in R^2$
	- Параметры:
		- `axis` - указывает, вдоль какой оси суммировать. Можно указать список осей [0, 1, ...]
		- `keepdims=True` - по-умолчанию редукция возвращает вектор-строку. True будет означать, что мы сохраняем информацию о направлении вектора.
```python
(tensor([[0., 1., 2.], 
		 [3., 4., 5.]])
).sum(axis=0)
# tensor([3., 5., 7.])
```
`A.mean()` - вычисление среднего значения всех элементов тензора (*mean, average*)
	- `A.sum() / A.numel()` - аналогично
	- `axis` - вычисление среднего по определённым осям
	  `A.mean(axis=0), A.sum(axis=0) / A.shape[0]`
- `A.cumsum(axis)` - кумулятивная сумма, суммирует элементы по указанной оси, складывая текущий элемент с предыдущими, но не с последующими. Сохраняет исходный shape. На примере понятнее:
```python
T.cumsum(axis=0)
# T -> tensor([[ 0, 1, 2], 
#              [ 3, 4, 5], 
#              [ 6, 7, 8], 
#              [ 9, 10, 11]])
# cumsum -> tensor([[ 0, 1, 2], 
#                   [ 3, 5, 7], 
#                   [ 9, 12, 15], 
#                   [18, 22, 26]])
```
# [[Нормы]]
#  Conversion to other Python Objects
> `type(object)` function for checking ***type***  
- `X.numpy()` - конвертация из `torch.Tensor` в `numpy.ndarray`
- `torch.from_numpy(A)` - конвертация из `numpy.ndarray` в `torch.Tensor`
	- `torch.tensor(A)` - аналогично
- `torch.tensor(df.to_numpy(dtype=float))` - преобразование: $pandas.DataFrame \rightarrow numpy.ndarray \rightarrow torch.Tensor$
- `a.item()` - если `a` - тензор-скаляр, то возвратится скаляр исходного в тензоре типа
- `float(a), int(a)` - преобразование тензор-скаляра `a` с помощью python-функций
---
# Autograd
> [[Calculus - Дифференциальное исчисление]]

> Требует повторения: https://d2l.ai/chapter_preliminaries/autograd.html

---
- Требуется место для хранения градиента. Способы указания:
	- параметр `required_grad=True`
	- атрибут `required_grad_`
```python
x = torch.arange(4.0, required_grad=True)
x.required_grad_(True)
```
- Вычисляется функция $x$, результат присваивается $y$
- Для вычисления градиента $y$ по отношению к $x$ вызывается метод `backward()`: `y.backward()`
	- `backward()` без параметров вычисляется только для ***скаляров***
	- параметр `gradient `(v) указывает, как свести объект с скаляру. 
		- Пример: 1. `y.backward(gradient=torch.ones(len(y)))`
		- 2. `y.sum().backward()` # Faster
	- Буфер градиентов $x$ не сбрасывается автоматически при записи нового градиента:
	  позволяет сбросить градиент: `x.grad.zero_()`
- Для получения доступа к градиенту $x$ вызывается атрибут `grad`: `x.grad`
## Detaching Computation
> Для отсоединения вычислений от вычислительного графа

- `y.detach()` - возвращает независимый объект (объект, предки которых были стёрты)

