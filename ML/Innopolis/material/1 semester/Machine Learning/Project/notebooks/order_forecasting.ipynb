{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from category_encoders import BinaryEncoder, cat_boost\n",
    "from sklearn import metrics, model_selection\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import builtins\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpfull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_casting(df_in):\n",
    "    df = df_in.copy()\n",
    "    cat_features = ['store_id','region_id','status_id']\n",
    "    float_feats = ['profit','order_price','max_price','min_price','avg_price','planned_prep_time']\n",
    "    int_feats = ['delivery_distance','products_count','unique_products_sold_by_store']\n",
    "    df[cat_features] = df[cat_features].astype('object')\n",
    "    df[float_feats] = df[float_feats].astype('float')\n",
    "    df[int_feats] = df[int_feats].astype('int')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplots_top_stores(df, columns, num_stores=5):\n",
    "    top_stores = df.value_counts('store_id').sort_values(ascending=False)[:num_stores].index\n",
    "\n",
    "    _, axes = plt.subplots(nrows=len(columns), \n",
    "                        ncols=num_stores+2, figsize=(num_stores*5, len(columns) * 3))\n",
    "    for idx, feature in enumerate(columns):\n",
    "        sns.histplot(df[feature], \n",
    "                    stat='density', \n",
    "                    kde=True,\n",
    "                    bins=35, ax=axes[idx, 0])\n",
    "        sns.boxplot(x=feature, data=df, ax=axes[idx, 1]).set_xlabel(feature + ' for all stores')\n",
    "        for shifted_idx, store_id in enumerate(top_stores, 2):\n",
    "            sns.boxplot(x=feature, \n",
    "                        data=df[df['store_id']==store_id], \n",
    "                        ax=axes[idx, shifted_idx]).set_xlabel(feature + ' for ' + str(store_id))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dates(df_in, date_columns):\n",
    "    df = df_in.copy()\n",
    "    mask_any_na_dates = df[date_columns].isna().any(axis=1)\n",
    "    df.loc[~mask_any_na_dates, date_columns] = np.sort(df.loc[~mask_any_na_dates, date_columns].values, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_outliers(df_in, features, quantiles: np.array, verbose=1):\n",
    "    assert len(features) == len(quantiles)\n",
    "    df = df_in.copy()\n",
    "    if verbose:\n",
    "        print('Shape before removing outliers: ', df.shape)\n",
    "    IQR = stats.iqr(df[features], axis=0, nan_policy='omit')\n",
    "\n",
    "    mask_non_outliers = df.groupby(\"store_id\")[features]\\\n",
    "        .apply(lambda x : (x>=(np.diag(x.quantile(quantiles[:,0])) - 1.5 * IQR)) \n",
    "                        & (x<=(np.diag(x.quantile(quantiles[:,1])) + 1.5 * IQR))\\\n",
    "              ).all(axis=1).values\n",
    "    df = df[mask_non_outliers]\n",
    "    if verbose:\n",
    "        print('Shape after removing outliers: ', df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_statistic(frame, method):\n",
    "    match method:\n",
    "        case 'mode':\n",
    "            return frame.mode()\n",
    "        case 'mean':\n",
    "            return frame.mean()\n",
    "        case 'median':\n",
    "            return frame.median()\n",
    "        case _:\n",
    "            raise NameError(\"Method is not exist\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_dates_by_store(x_in, method):\n",
    "    x = x_in.copy()\n",
    "    # calculate statistics value for recover null observation in regarding the neighbour date feature\n",
    "    diff_date = x['order_pickup'] - x['order_ready']\n",
    "    diff_timestamp_statistic = calc_statistic(diff_date[diff_date > pd.to_timedelta(\"0 days\")], method)    \n",
    "\n",
    "    # From EDA, this column hasn't null rows, but to be sure this code will be left \n",
    "    # order_pickup = order_ready + method(order_pickup - order_ready)\n",
    "    mask_null_order_pickup = x['order_pickup'].isnull() \n",
    "    x.loc[mask_null_order_pickup, 'order_pickup'] = \\\n",
    "        x.loc[mask_null_order_pickup, 'order_ready'] + diff_timestamp_statistic\n",
    "    \n",
    "    # order_ready = order_pickup - method(order_pickup - order_ready)\n",
    "    mask_null_order_ready = x['order_ready'].isnull()\n",
    "    x.loc[mask_null_order_ready, 'order_ready'] = \\\n",
    "        x.loc[mask_null_order_ready, 'order_pickup'] - diff_timestamp_statistic\n",
    "\n",
    "    # New feature that relfects waiting time for the order to be accepted for processing\n",
    "    x['order_delay'] = x['order_start_prepare'] - x['date_create']\n",
    "    diff_timestamp_statistic = calc_statistic(x['order_delay'][x['order_delay'] > pd.to_timedelta(\"0 days\")], method)    \n",
    "    x['order_delay'] = x['order_delay'].dt.total_seconds() / 60\n",
    "\n",
    "    # order_start_prepare = date_create + method(order_start_prepare - date_create)\n",
    "    mask_null_order_start_prepare = x['order_start_prepare'].isnull()\n",
    "    x.loc[mask_null_order_start_prepare, 'order_start_prepare'] = \\\n",
    "        x.loc[mask_null_order_start_prepare, 'date_create'] + diff_timestamp_statistic\n",
    "    \n",
    "    # date_create = order_start_prepare - method(order_start_prepare - date_create)\n",
    "    mask_null_date_create = x['date_create'].isnull()\n",
    "    x.loc[mask_null_date_create, 'date_create'] = \\\n",
    "        x.loc[mask_null_date_create, 'order_start_prepare'] - diff_timestamp_statistic\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_dates(df_in, method='mode'):\n",
    "    df = df_in.copy()\n",
    "    # Restore dates within a certain store_id by method of differences neighboor columns \n",
    "    # (order_pickup - order_ready) & (order_start_prepare - date_create)\n",
    "    df.loc[:,['date_create','order_start_prepare','order_ready','order_pickup','order_delay']] = df.groupby(\"store_id\")\\\n",
    "            .apply(lambda x: impute_dates_by_store(x, method))\\\n",
    "            .reset_index(allow_duplicates=True)\\\n",
    "            .set_index('level_1')[['date_create','order_start_prepare','order_ready','order_pickup','order_delay']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_target(df_in, q_target=0.75, test_subset=False, verbose=1):\n",
    "    df = df_in.copy()\n",
    "    df['actual_prep_time'] = (df['order_ready'] - df['order_start_prepare']).dt.total_seconds() / 60\n",
    "    \n",
    "    mask_null_prep_time = df['actual_prep_time'].isnull() \n",
    "    if not test_subset:\n",
    "        df['diff_time'] = df['actual_prep_time'] - df['planned_prep_time']\n",
    "        \n",
    "        mask_neg_prep_time = df['actual_prep_time'] < 0\n",
    "        IQR = stats.iqr(df['diff_time'], nan_policy='omit')\n",
    "        mask_outliers_in_diff_time = df['diff_time'] > (df['diff_time'].quantile(q_target) + 1.5 * IQR)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Nan's in actual_prep_time: \", mask_null_prep_time.sum(),\n",
    "                \"\\nNegatives: \", mask_neg_prep_time.sum(),\n",
    "                '\\nOutliers in time difference: ', mask_outliers_in_diff_time.sum())\n",
    "        \n",
    "        df.drop(columns='diff_time', inplace=True)\n",
    "        df.dropna(subset=['date_create','planned_prep_time'], inplace=True)\n",
    "        mask_anomaly = mask_neg_prep_time | mask_null_prep_time | mask_outliers_in_diff_time\n",
    "    else:\n",
    "        mask_anomaly = mask_null_prep_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"All anomalyes: \", df.loc[mask_anomaly].shape[0])\n",
    "    df = df.loc[~mask_anomaly]\n",
    "    df['on_time'] = np.where(abs(df['planned_prep_time'] - df['actual_prep_time']) <= 5, 1, 0)\n",
    "    return df.drop(columns=['order_ready','order_pickup','actual_prep_time', 'on_time']), df['on_time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cyclical_feat(df_in: pd.DataFrame, col: pd.Series, col_name: str):\n",
    "    df = df_in.copy()\n",
    "    df[col_name + '_sin'] = np.sin(2 * np.pi * col/col.max())\n",
    "    df[col_name + '_cos'] = np.cos(2 * np.pi * col/col.max())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_feat(df_in: pd.DataFrame, data_features: list, drop=True, encode_cyclical=True):\n",
    "    df = df_in.copy()\n",
    "    if encode_cyclical:\n",
    "        for col_name in data_features:\n",
    "            df = encode_cyclical_feat(df, df[col_name].dt.month, col_name + '_month')\n",
    "            df = encode_cyclical_feat(df, df[col_name].dt.day, col_name + '_day')\n",
    "            df = encode_cyclical_feat(df, df[col_name].dt.weekday, col_name + '_weekday')\n",
    "            df = encode_cyclical_feat(df, df[col_name].dt.hour, col_name + '_hour')\n",
    "            df = encode_cyclical_feat(df, df[col_name].dt.minute, col_name + '_minute')\n",
    "    else:\n",
    "        for col_name in data_features:\n",
    "            df[col_name + '_month'] = df[col_name].dt.month\n",
    "            df[col_name + '_day'] = df[col_name].dt.day\n",
    "            df[col_name + '_weekday'] = df[col_name].dt.weekday\n",
    "            df[col_name + '_hour'] = df[col_name].dt.hour\n",
    "            df[col_name + '_minute'] = df[col_name].dt.minute\n",
    "            \n",
    "    if drop:\n",
    "        df = df.drop(columns=data_features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_imputer(X_train, X_test, y_train, y_test, restore_numerical_nans=True, \n",
    "                   imputing_method_for_numerical='mode', \n",
    "                   imputing_method_for_simple_imputer='most_frequent', verbose=1):\n",
    "    na_columns_train = X_train.columns[X_train.isna().any()]\n",
    "    na_columns_test = X_test.columns[X_test.isna().any()]\n",
    "\n",
    "    if verbose: \n",
    "        print(\"Numbers of NaN values\\n\\tTrain:\\n\", X_train[na_columns_train].isna().sum())\n",
    "        print(\"\\n\\tTest:\\n\", X_test[na_columns_test].isna().sum())\n",
    "\n",
    "    if restore_numerical_nans:\n",
    "        X_train.loc[:,na_columns_train] = X_train.groupby(\"store_id\")\\\n",
    "            [na_columns_train].transform(lambda x: x.fillna(calc_statistic(x, imputing_method_for_numerical)))\n",
    "        X_test.loc[:,na_columns_test] = X_test.groupby(\"store_id\")\\\n",
    "            [na_columns_test].transform(lambda x: x.fillna(calc_statistic(x, imputing_method_for_numerical)))\n",
    "\n",
    "        ####################################### Imputting #####################################\n",
    "        col_imputing = X_test.columns[X_test.isna().any()].union(X_train.columns[X_train.isna().any()])\n",
    "        if verbose:\n",
    "            print(\"Remaining numbers of NaN values\\n\\tTrain:\\n\", X_train[col_imputing].isna().sum())\n",
    "            print(\"\\n\\tTest:\\n\", X_test[col_imputing].isna().sum())\n",
    "\n",
    "        imputer = SimpleImputer(strategy=imputing_method_for_simple_imputer)\n",
    "        X_train[col_imputing] = pd.DataFrame(imputer.fit_transform(X_train[col_imputing]), \n",
    "                                            columns=col_imputing, index=X_train.index)\n",
    "        X_test[col_imputing] = pd.DataFrame(imputer.transform(X_test[col_imputing]), \n",
    "                                            columns=col_imputing, index=X_test.index)\n",
    "    else:\n",
    "        index_train_na, index_test_na = X_train[na_columns_train].isnull().any(axis=1), \\\n",
    "                                        X_test[na_columns_test].isnull().any(axis=1)    \n",
    "        X_train, y_train, X_test, y_test = X_train[~index_train_na], y_train[~index_train_na], \\\n",
    "                                        X_test[~index_test_na], y_test[~index_test_na]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(estimator, X_test, y_test, X_train, y_train, cv=5, cv_scoring=['f1'], threshold=0.5, plot=True, ax=None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists\n",
    "from pathlib import Path\n",
    "\n",
    "COLORMAP = [[0.0, '#3f7f93'],\n",
    "            [0.1, '#6397a7'],\n",
    "            [0.2, '#88b1bd'],\n",
    "            [0.3, '#acc9d2'],\n",
    "            [0.4, '#d1e2e7'],\n",
    "            [0.5, '#f2f2f2'],\n",
    "            [0.6, '#f6cdd0'],\n",
    "            [0.7, '#efa8ad'],\n",
    "            [0.8, '#e8848b'],\n",
    "            [0.9, '#e15e68'],\n",
    "            [1.0, '#da3b46']]\n",
    "\n",
    "PLOT_THEME='plotly_dark'\n",
    "# PLOT_THEME='none'\n",
    "VERBOSE=False\n",
    "SHOW_PLOTS=False\n",
    "APPEND_TO_EXISTS=False\n",
    "DROP_OLD_COLUMNS=True\n",
    "TARGET='Добыча воды за 2 ч ,м3, лаг -1'\n",
    "TIME_AXIS='YY-MM-DD HH:00'\n",
    "INSERT_NEARBY=True\n",
    "LOG=(False, False) # log_x=LOG[0], log_y=LOG[1]\n",
    "TEXTFONT_SIZE=10\n",
    "FILEPATH=None\n",
    "\n",
    "ROOT = Path('/home/prog3/innopolis/ML/assignment/')\n",
    "DATA = join(ROOT, 'content')\n",
    "PLOTS = join(ROOT, 'plots')\n",
    "MODELS = join(ROOT, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from phik.report import plot_correlation_matrix\n",
    "from scipy import signal\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# slice функция по индексу для словарей\n",
    "slice = lambda d, start=0, stop=None, step=1: dict(itertools.islice(d.items(), start, stop, step))\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    #region base\n",
    "    def __init__(self, df: pd.DataFrame=None, name='dataset', verbose=VERBOSE, features_names=None, targets_names=None):\n",
    "        self.verbose = verbose\n",
    "        self.name = name\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            self.df = df.copy()\n",
    "            self.original = df.copy()\n",
    "\n",
    "        self.features_names = features_names\n",
    "        self.targets_names = targets_names\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data.loc[index]\n",
    "        \n",
    "\n",
    "    def load(self, path, dropna=True, parse_dates=None):\n",
    "        print(\"Чтение датасета...\") if self.verbose else None\n",
    "        match path.split('.')[-1]:\n",
    "            case 'xlsx':\n",
    "                self.df: pd.DataFrame = pd.read_excel(path, parse_dates=parse_dates)\n",
    "            case 'csv':\n",
    "                self.df: pd.DataFrame = pd.read_csv(path, parse_dates=parse_dates)\n",
    "        \n",
    "        self.original = self.df.copy()\n",
    "        self.df.columns = [col.replace('|', ',') for col in self.df]\n",
    "        if dropna:\n",
    "            print(f\"Удаленые записи\\n{self.df.isna().sum()}\") if self.verbose else None\n",
    "            self.df = self.df.dropna().reset_index(drop=True)\n",
    "        print(\"Завершено успешно\") if self.verbose else None\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.cols.__iter__()\n",
    "    \n",
    "    def astype(self, columns, type):\n",
    "        columns = self.columns(columns)\n",
    "        for col in columns:\n",
    "            self.df[col] = self.df[col].astype(type)\n",
    "    \n",
    "    @property\n",
    "    def cols(self):\n",
    "        return self.df.columns\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.df.loc[:, self.features_names] \\\n",
    "            if self.features_names else self.df.iloc[:,1:-1]\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def target(self):\n",
    "        return self.df.loc[:, self.targets_names] \\\n",
    "            if self.targets_names else self.df.iloc[:,-1]\n",
    "    \n",
    "\n",
    "    def columns(self, pattern: list|str):\n",
    "        match type(pattern):\n",
    "            case builtins.list | pd.Index:\n",
    "                return [col for col in pattern if col in self.cols]\n",
    "            case builtins.str:\n",
    "                return [col for col in self.df if re.search(pattern, col)]\n",
    "            case _:\n",
    "                if pattern==None:\n",
    "                    return self.cols\n",
    "                else:\n",
    "                    raise TypeError(\"Передан некорректный параметр 'pattern'\")\n",
    "                \n",
    "    def drop(self, columns):\n",
    "        columns = self.columns(columns)\n",
    "        self.df.drop(columns, axis=1, inplace=True)\n",
    "    \n",
    "    def save(self, filename=None, extension='csv'):\n",
    "        match extension:\n",
    "            case 'csv':\n",
    "                self.df.to_csv(filename, index=False)\n",
    "            case 'xslx':\n",
    "                self.df.to_excel(filename, index=False)\n",
    "\n",
    "    def __insert_or_replace(self, column, insert, method, data):\n",
    "        if insert:\n",
    "            index = self.cols.get_loc(column) + 1\n",
    "            new_col = column \\\n",
    "                    if column.endswith(method) \\\n",
    "                    else f'smoothed,{column},{method}'\n",
    "            print('Вставка нового столбца: ', new_col) if self.verbose else None\n",
    "            try:\n",
    "                self.df.insert(index, new_col, data)\n",
    "            except ValueError as err:\n",
    "                print(err) if self.verbose else None\n",
    "                self.df.loc[:, new_col] = data\n",
    "        else:\n",
    "            self.df.loc[:, column] = data\n",
    "\n",
    "    #endregion\n",
    "\n",
    "    #region time processing\n",
    "    def parse_datetime(self, parse_date=True, drop_date=DROP_OLD_COLUMNS, \n",
    "                       parse_time=True, drop_time=DROP_OLD_COLUMNS, \n",
    "                       replace_time_0_to_24=False):\n",
    "        \"\"\"\n",
    "                Расчленяет фичу \"Дата\" на 3 колонки: ['Год','Месяц','День']\n",
    "                и преобразует фичу \"Время\" (datetime) в фичу \"Час\" (int)\n",
    "            parse_date: bool (default True) - флаг на разрешение расчленения фичи \"Дата\"\n",
    "            drop_date: bool (default True) - флаг на удаление фичи \"Дата\" после расчленения\n",
    "            parse_time: bool (default True) - флаг на разрешение преобразования фичи \"Время\"\n",
    "            drop_time: bool (default True) - флаг на удаление фичи \"Время\" после преобразования\n",
    "            replace_time_0_to_24: bool (default False) - флаг замены 0 (полночь) на 24\n",
    "        \"\"\"\n",
    "        print(\"Парсинг Даты и Времени...\") if self.verbose else None\n",
    "        if parse_date:\n",
    "            loc=self.cols.get_loc('Дата')\n",
    "            self.df.insert(loc, 'День', self.df['Дата'].dt.day)\n",
    "            self.df.insert(loc, 'Месяц', self.df['Дата'].dt.month)\n",
    "            self.df.insert(loc, 'Год', self.df['Дата'].dt.year)\n",
    "            \n",
    "        if parse_time:\n",
    "            self.df.insert(self.cols.get_loc('Время'), 'Час', self.df['Время'].apply(lambda x: x.hour))\n",
    "            if replace_time_0_to_24:\n",
    "                self.df.loc[self.df['Час'] == 0, 'Час'] = 24\n",
    "        if drop_date:\n",
    "            self.df.drop(columns='Дата', inplace=True)\n",
    "        if drop_time:\n",
    "            self.df.drop(columns='Время', inplace=True)\n",
    "\n",
    "    def set_lag(self, lag=0, drop=DROP_OLD_COLUMNS, target='Добыча воды за 2 ч ,м3'):\n",
    "        \"\"\"\n",
    "                Добавления смещения значений по оси 0 (по строкам)\n",
    "                Отрительные значения -> смещение вверх, положительные -> вниз\n",
    "            lag: (default 0) int - значение смещение (лага, шага) по строкам\n",
    "            drop: bool (default False) - флаг на удаление смещаемой фичи (оригинальной)\n",
    "            target: str (default Добыча воды за 2 ч ,м3) - название фичи для сдвига\n",
    "        \"\"\"\n",
    "        print(f\"Смещение признака '{target}' на lag {lag}\") if self.verbose else None\n",
    "        if lag:\n",
    "            self.df = pd.concat([self.df, \n",
    "                                self.df[target].shift(lag).\n",
    "                                rename(f\"{target}, лаг {lag}\")], axis=1).dropna().reset_index(drop=True)\n",
    "            if drop:\n",
    "                self.df.drop(columns=target, inplace=True)\n",
    "            \n",
    "            print(f\"'{target}' смещено на {lag}\") if self.verbose else None\n",
    "        else:\n",
    "            print('Смещение установлено в 0. Пропускаем...') if self.verbose else None\n",
    "\n",
    "    def filter_by_hours(self, hours: list=[8, 20]):\n",
    "        \"\"\"\n",
    "                Фильтрует датасет по `hours` часам\n",
    "                Необходим признак 'Час'\n",
    "                *Метод parse_datetime достает фичу 'Час'*\n",
    "            hours: list (default [8, 20]) - int значения часов для фильтрации\n",
    "        \"\"\"\n",
    "        print(f\"Фильтрация по {hours} часам\") if self.verbose else None\n",
    "        if 'Час' in self.cols:\n",
    "            self.df = self.df[self.df['Час'].isin(hours)].reset_index(drop=True)\n",
    "            print(self.df['Час'].value_counts()) if self.verbose else None\n",
    "        else:\n",
    "            print(\"Признак 'Час' не найден\")\n",
    "\n",
    "    def convert_datetime(self, drop=DROP_OLD_COLUMNS):\n",
    "        datetimes = ['Год', 'Месяц','День','Час']\n",
    "        self.df.insert(0, TIME_AXIS, self.df[datetimes].apply(\n",
    "            lambda x: f'{x.iloc[0]:04}-{x.iloc[1]:02}-{x.iloc[2]:02} {x.iloc[3]:02}:00', \n",
    "            axis = 1)) \n",
    "        self.df.sort_values(TIME_AXIS, inplace=True)\n",
    "        if drop:\n",
    "            self.df.drop(columns=datetimes, inplace=True)\n",
    "\n",
    "    #endregion\n",
    "\n",
    "    #region preprocessing\n",
    "    def new_feature(self, columns, newcol, agg_f:str='max', drop=DROP_OLD_COLUMNS):\n",
    "        columns = self.columns(columns)\n",
    "        match agg_f:\n",
    "            case 'max':\n",
    "                self.df[newcol] = self.df[columns].max(axis=1)\n",
    "            case 'median':\n",
    "                self.df[newcol] = self.df[columns].median(axis=1)\n",
    "            case 'mean':\n",
    "                self.df[newcol] = self.df[columns].mean(axis=1)\n",
    "        if drop:\n",
    "            self.drop(columns)\n",
    "\n",
    "\n",
    "    def recovery_outliers(self, columns, save_interval, insert=True):\n",
    "    \n",
    "        columns = self.columns(columns)\n",
    "        print(\"Восстановление выбросов...\") if self.verbose else None\n",
    "        for idx, col in enumerate(columns):\n",
    "            mask = (\n",
    "                    (self.df[col].between(*save_interval) )\n",
    "                )\n",
    "            \n",
    "            data = pd.Series(np.where(mask, self.df[col], np.nan))\n",
    "            print(\"Выбросы: \", self.df.loc[~mask, col]) if self.verbose else None\n",
    "            data = data.ffill()\n",
    "            self.__insert_or_replace(col, insert, 'outlier', data)\n",
    "            \n",
    "\n",
    "\n",
    "    def smooth(self, columns, frac=0.001, window=5, polyorder=3, insert=INSERT_NEARBY, method='lowess'):\n",
    "        \"\"\"\n",
    "            Сглаживание значений в указанных признаках\n",
    "            Поддерживаемые методы:\n",
    "            - lowess - Сглаживание по методу Лоусса\n",
    "            - rolling - Скользящее среднее окно\n",
    "            - savgol_filter (https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter) - Фильтр Савицкого-Голея \n",
    "                - подгоняет последующие окна смежных данных с полиномом низкого порядка\n",
    "        \"\"\"\n",
    "        columns = self.columns(columns)\n",
    "        print(\"Выполняется сглаживание значений рядов: \", columns) if self.verbose else None\n",
    "        for col in columns:\n",
    "            match method:\n",
    "                case 'lowess':\n",
    "                    smoothed = lowess(self.df[col], range(len(self.df)), frac=frac)[:, 1]\n",
    "                case 'rolling':\n",
    "                    smoothed = self.df[col].rolling(window=window).mean()\n",
    "                case 'savgol_filter':\n",
    "                    smoothed = signal.savgol_filter(self.df[col],\n",
    "                               window, # window size used for filtering\n",
    "                               polyorder) # order of fitted polynomial\n",
    "                case _:\n",
    "                    raise ValueError(\"Указан неподдерживаемый метод\")\n",
    "\n",
    "            self.__insert_or_replace(col, insert, method, smoothed)\n",
    "           \n",
    "\n",
    "    def zscore(self, s, window, thresh=3, return_all=False, coeff=1.0):\n",
    "        roll = s.rolling(window=window, min_periods=1)\n",
    "        avg = roll.mean()\n",
    "        max = roll.max()\n",
    "        min = roll.min()\n",
    "        std = roll.std(coeff)\n",
    "        z = s.sub(avg).div(std)   \n",
    "        m = z.between(-thresh, thresh)\n",
    "        \n",
    "        if return_all:\n",
    "            return s.where(m, avg), max, min\n",
    "        return s.where(m, avg)\n",
    "\n",
    "    def scale(self, columns, method='standard', insert=False):\n",
    "        columns = self.columns(columns)\n",
    "        print(\"Выполняется масштабирование значений признаков: \", columns) if self.verbose else None\n",
    "        match method:\n",
    "            case 'standard':\n",
    "                scaled = preprocessing.StandardScaler().fit_transform(self.df[columns])\n",
    "            case 'minmax':\n",
    "                scaled = preprocessing.MinMaxScaler().fit_transform(self.df[columns])\n",
    "            case _:\n",
    "                raise ValueError(\"Указан неподдерживаемый метод\")\n",
    "        self.__insert_or_replace(columns, insert, method, scaled)\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "    #endregion\n",
    "\n",
    "    #region plots \n",
    "    @staticmethod \n",
    "    def plot_template(fig: go.Figure, filepath=FILEPATH, show=SHOW_PLOTS, append=APPEND_TO_EXISTS, \n",
    "                      verbose=VERBOSE, mlflow_track=False, run_id=None, **layout_params):\n",
    "        fig.update_layout(template=PLOT_THEME, **layout_params)\n",
    "        if filepath:\n",
    "            filepath = filepath if filepath.endswith('.html') else f'{filepath}.html'\n",
    "            if not os.path.exists(filepath) or not append:\n",
    "                fig.write_html(filepath)\n",
    "            else:\n",
    "                with open(filepath, 'a') as f:\n",
    "                    f.write(fig.to_html(full_html=False, include_plotlyjs=False))\n",
    "            print(\"Файл сгенерирован: \", filepath) if verbose else None\n",
    "        if show:\n",
    "            fig.show()\n",
    "        if mlflow_track:\n",
    "            mlflow.log_artifact(local_path=filepath, run_id=run_id)\n",
    "\n",
    "    def report(self, columns=None, filepath=FILEPATH, show=SHOW_PLOTS):\n",
    "        columns = self.columns(columns)\n",
    "        report = ProfileReport(self.df[columns], \n",
    "                      title=\"Profiling Report\")\n",
    "        if filepath:\n",
    "            report.to_file(output_file=filepath)\n",
    "        if show:\n",
    "            return report    \n",
    "\n",
    "    def time_series(self, columns=None, appendix_cols=None, log=LOG, time_axis=TIME_AXIS,\n",
    "                    **kwargs): # filepath, show, height, width, append, title and other (layout_params)\n",
    "        columns = self.columns(columns)\n",
    "        if appendix_cols:\n",
    "            columns += appendix_cols\n",
    "        if time_axis not in self.cols:\n",
    "            columns.append(time_axis)\n",
    "        print(\"График отображает признаки: \", columns) if self.verbose else None\n",
    "    \n",
    "        try:\n",
    "            fig = px.line(self.df, x=time_axis, y=columns,\n",
    "                          log_x=log[0], log_y=log[1])\n",
    "            self.plot_template(fig, **kwargs)\n",
    "        except BaseException as err:\n",
    "            print(err)\n",
    "\n",
    "        \n",
    "    def scatter_matrix(self, columns=None, **kwargs):\n",
    "        columns = self.columns(columns)\n",
    "        fig = px.scatter_matrix(self.df[columns])\n",
    "        fig.update_traces(diagonal_visible=False, showlowerhalf=False)\n",
    "        self.plot_template(fig, **kwargs)\n",
    "\n",
    "    def difference_with_smoothed(self, column, time_axis=TIME_AXIS, method='rolling',mode='markers',\n",
    "                                 **kwargs): # filepath, show, height, width, append, title and other (layout_params)\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=self.df[time_axis],\n",
    "            y=self.df[column],\n",
    "            marker=dict(size=2, color='black',),\n",
    "            opacity=0.25,\n",
    "            name=column\n",
    "        ))\n",
    "        xaxis = self.df[time_axis]\n",
    "        try:\n",
    "            ylabel = self.columns(column+\",\"+method)[0]\n",
    "            yaxis = self.df[ylabel]\n",
    "        except BaseException as err:\n",
    "            print(err, f'ylabel: {self.columns(column)}')\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=xaxis,\n",
    "            y=yaxis,\n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color='royalblue',\n",
    "                symbol='circle-open'\n",
    "            ),\n",
    "            name=ylabel\n",
    "        ))\n",
    "\n",
    "        if mode:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=xaxis,\n",
    "                y=yaxis,\n",
    "                mode=mode,\n",
    "                marker=dict(\n",
    "                    size=6,\n",
    "                    color='mediumpurple',\n",
    "                    symbol='triangle-up'\n",
    "                ),\n",
    "                name='Smoothed scatter'\n",
    "            ))\n",
    "\n",
    "        self.plot_template(fig, **kwargs)\n",
    "\n",
    "\n",
    "    def corr_matrix(self, columns=None, target=TARGET, \n",
    "                    method='pearson', textfont_size=TEXTFONT_SIZE, \n",
    "                    **kwargs): # filepath, show, height, width, append, title and other (layout_params)\n",
    "        columns = self.columns(columns)\n",
    "        if target not in columns:\n",
    "            columns.append(target)\n",
    "       \n",
    "        if target:\n",
    "            corr = pd.concat([pd.DataFrame(self.df[columns].corr('pearson')[target]).rename(columns={target:'pearson'}).T,\n",
    "                            pd.DataFrame(self.df[columns].corr('kendall')[target]).rename(columns={target:'kendall'}).T,\n",
    "                            pd.DataFrame(self.df[columns].corr('spearman')[target]).rename(columns={target:'spearman'}).T,\n",
    "                            pd.DataFrame(self.df[columns].phik_matrix()[target]).rename(columns={target:'phik'}).T]) \n",
    "        elif method=='phik': \n",
    "            corr = self.df[columns].phik_matrix()\n",
    "        else:\n",
    "            corr = self.df[columns].corr(method) \n",
    "       \n",
    "        fig = px.imshow(corr, text_auto=True,  color_continuous_scale=COLORMAP)\n",
    "        fig.update_traces(textfont_size=textfont_size,  texttemplate = \"%{z:.2f}\")\n",
    "        self.plot_template(fig, **kwargs)\n",
    "\n",
    "    #endregion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['date_create', 'order_start_prepare', 'order_ready', 'order_pickup']\n",
    "orders = pd.read_csv('../content/aggregated_df.csv', parse_dates=date_columns)\n",
    "orders = type_casting(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['store_id', 'profit', 'delivery_distance', 'date_create',\n",
       "       'order_start_prepare', 'planned_prep_time', 'order_ready',\n",
       "       'order_pickup', 'region_id', 'status_id', 'products_count',\n",
       "       'order_price', 'max_price', 'min_price', 'avg_price',\n",
       "       'unique_products_sold_by_store'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "all inputs must be Index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 303\u001b[0m, in \u001b[0;36mDataset.time_series\u001b[0;34m(self, columns, appendix_cols, log, time_axis, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     columns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m appendix_cols\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_axis \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcols:\n\u001b[0;32m--> 303\u001b[0m     \u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_axis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mГрафик отображает признаки: \u001b[39m\u001b[38;5;124m\"\u001b[39m, columns) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/assignment-nL_vQZMu-py3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:5484\u001b[0m, in \u001b[0;36mIndex.append\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   5482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m to_concat:\n\u001b[1;32m   5483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, Index):\n\u001b[0;32m-> 5484\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall inputs must be Index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5486\u001b[0m names \u001b[38;5;241m=\u001b[39m {obj\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m to_concat}\n\u001b[1;32m   5487\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[0;31mTypeError\u001b[0m: all inputs must be Index"
     ]
    }
   ],
   "source": [
    "df.time_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of 'x' is not the name of a column in 'data_frame'. Expected one of ['store_id', 'profit', 'delivery_distance', 'date_create', 'order_start_prepare', 'planned_prep_time', 'order_ready', 'order_pickup', 'region_id', 'status_id', 'products_count', 'order_price', 'max_price', 'min_price', 'avg_price', 'unique_products_sold_by_store'] but received: YY-MM-DD HH:00\n"
     ]
    }
   ],
   "source": [
    "df.time_series(columns=df.cols,log=(False,False), show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTLIER DETECTION & FUTURE ORDERS FORECASTING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
