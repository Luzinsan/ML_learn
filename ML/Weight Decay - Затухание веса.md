> Метод регуляризации параметрических моделей, позволяющий манипулировать числом параметров, путём ограничения значений, которые могут принимать параметры - $l_2$ регуляризация.
> Для решения задачи нужно измерить расстояние между функцией и нулём:
> => задаче посвящены разделы функционального анализа и теории банаховых пространств (**Banach spaces**)

На примере линейной функции $f(\mathbf{x}) = \mathbf{w}^T\mathbf{x}$:
- Норму вектора можно взять: $||\mathbf{w}||^2$ , где $||\mathbf{w}||$ - $l_2$ норма
- Способ обеспечения вектора весов с малыми коэффициентами: добавление его нормы в качестве штрафного члена в процессе минимизации loss функции. 
- Было: минимизация прогнозов loss функции (*prediction loss*) на тренировочных метках.
- Стало: минимизация суммы прогнозов loss функции (*prediction loss*) и штрафного срока (*penalty term*).
> Loss функция для линейной регрессии: [[Linear Regression#Функция потерь - Loss Function]]
> Добавление штрафа: $L(\mathbf{w},b) + \frac{\lambda}{2}||\mathbf{w}||^2$
>> Объяснение формулы: когда мы берем производную квадратичной функции, степень 2 и скаляр $\frac{1}{2}$ сокращаются. $||\mathbf{w}||^2 = \left(\sqrt{w_1^2 + w_2^2 + ... + w_d^2}\right)^2 = {w_1^2 + w_2^2 + ... + w_d^2}$ Остаётся $\frac{\lambda}{2} ||\mathbf{w}||^2 = \frac{\lambda}{2}{(w_1^2 + w_2^2 + ... + w_d^2)}$
>> Производная: $\lambda(w_1 + w_2 + ... +w_d )$

# Регуляризация
- $l_2$-регуляризационные модели - алгоритмы Ridge Regression - равномерно распределяет вес по всем признакам [[Linear Regression#Minibatch Stochastic Gradient Descent]]
	- Большее значение $\lambda$ характеризует более агрессивное уменьшение весов $\mathbf{w}$
	- Меньшее $\lambda$, наоборот, слабее накладывает штраф весам $\mathbf{w}$
> $$\begin{aligned}
\mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).
\end{aligned}$$
- $l_1$-регуляризационные модели - алгоритмы Lasso Regression - концентрирует веса на маленьком наборе признаков, зануляя все остальные веса - эффективный способ *feature selection*.